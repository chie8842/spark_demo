from pyspark import SparkContext

data = sc.textFile("hdfs:///user/hadoop/housing.data")

data2 = data.map(lambda x: [float(i) for i in x.strip().split()])
(spark.createDataFrame(
        data2, 
        ["CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT", "MEDV"])
        .show())

# +-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+
# |   CRIM|  ZN|INDUS|CHAS|  NOX|   RM|  AGE|   DIS|RAD|  TAX|PTRATIO|     B|LSTAT|MEDV|
# +-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+
# |0.00632|18.0| 2.31| 0.0|0.538|6.575| 65.2|  4.09|1.0|296.0|   15.3| 396.9| 4.98|24.0|
# |0.02731| 0.0| 7.07| 0.0|0.469|6.421| 78.9|4.9671|2.0|242.0|   17.8| 396.9| 9.14|21.6|
# |0.02729| 0.0| 7.07| 0.0|0.469|7.185| 61.1|4.9671|2.0|242.0|   17.8|392.83| 4.03|34.7|
# |0.03237| 0.0| 2.18| 0.0|0.458|6.998| 45.8|6.0622|3.0|222.0|   18.7|394.63| 2.94|33.4|
# |0.06905| 0.0| 2.18| 0.0|0.458|7.147| 54.2|6.0622|3.0|222.0|   18.7| 396.9| 5.33|36.2|
# |0.02985| 0.0| 2.18| 0.0|0.458| 6.43| 58.7|6.0622|3.0|222.0|   18.7|394.12| 5.21|28.7|
# |0.08829|12.5| 7.87| 0.0|0.524|6.012| 66.6|5.5605|5.0|311.0|   15.2| 395.6|12.43|22.9|
# |0.14455|12.5| 7.87| 0.0|0.524|6.172| 96.1|5.9505|5.0|311.0|   15.2| 396.9|19.15|27.1|
# |0.21124|12.5| 7.87| 0.0|0.524|5.631|100.0|6.0821|5.0|311.0|   15.2|386.63|29.93|16.5|
# |0.17004|12.5| 7.87| 0.0|0.524|6.004| 85.9|6.5921|5.0|311.0|   15.2|386.71| 17.1|18.9|
# |0.22489|12.5| 7.87| 0.0|0.524|6.377| 94.3|6.3467|5.0|311.0|   15.2|392.52|20.45|15.0|
# |0.11747|12.5| 7.87| 0.0|0.524|6.009| 82.9|6.2267|5.0|311.0|   15.2| 396.9|13.27|18.9|
# |0.09378|12.5| 7.87| 0.0|0.524|5.889| 39.0|5.4509|5.0|311.0|   15.2| 390.5|15.71|21.7|
# |0.62976| 0.0| 8.14| 0.0|0.538|5.949| 61.8|4.7075|4.0|307.0|   21.0| 396.9| 8.26|20.4|
# |0.63796| 0.0| 8.14| 0.0|0.538|6.096| 84.5|4.4619|4.0|307.0|   21.0|380.02|10.26|18.2|

from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import VectorAssembler
from pyspark.mllib.regression import LabeledPoint
from pyspark.ml.regression import LinearRegression
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.feature import PolynomialExpansion
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml import Pipeline

data3 = data2.map(lambda x: [x[-1],Vectors.dense(x[0:len(x)-2])])
data4 = spark.createDataFrame(data3, ["label", "features"])
data4.show(5, truncate=False)

# +-----+--------------------------------------------------------------------+
# |label|features                                                            |
# +-----+--------------------------------------------------------------------+
# |24.0 |[0.00632,18.0,2.31,0.0,0.538,6.575,65.2,4.09,1.0,296.0,15.3,396.9]  |
# |21.6 |[0.02731,0.0,7.07,0.0,0.469,6.421,78.9,4.9671,2.0,242.0,17.8,396.9] |
# |34.7 |[0.02729,0.0,7.07,0.0,0.469,7.185,61.1,4.9671,2.0,242.0,17.8,392.83]|
# |33.4 |[0.03237,0.0,2.18,0.0,0.458,6.998,45.8,6.0622,3.0,222.0,18.7,394.63]|
# |36.2 |[0.06905,0.0,2.18,0.0,0.458,7.147,54.2,6.0622,3.0,222.0,18.7,396.9] |
# +-----+--------------------------------------------------------------------+
# only showing top 5 rows


assembler = VectorAssembler(inputCols=["features"], outputCol="assembled")

# 交互作用項の追加
pe = PolynomialExpansion().setInputCol("features").setOutputCol("polyfeatures")

regressor = LinearRegression().setStandardization(False).setSolver("l-bfgs").setLabelCol("label")

# パラメータチューニングの設定
paramGrid = (ParamGridBuilder()
        .addGrid(pe.degree, [2,3])
        .addGrid(regressor.maxIter, [10,25,50])
        .addGrid(regressor.regParam, [0.0, 0.01, 0.1])
        .addGrid(regressor.featuresCol, ["features", "features", "polyFeatures"])
        .addGrid(regressor.featuresCol, ["polyfeatures"])
        .build())

# 評価にはRMSEを使う
evaluator = RegressionEvaluator(metricName="rmse")

# 交差検定モデルの作成
model = (CrossValidator()
    .setEstimator(Pipeline(stages=[assembler, pe, regressor]))
    .setEvaluator(evaluator)
    .setEstimatorParamMaps(paramGrid)
    .setNumFolds(5)
    .fit(data4))

print("best metrics is",  min(model.avgMetrics))

## best metrics is 4.5443596397773

model.bestModel.transform(data4).show()

# +-----+--------------------+--------------------+--------------------+------------------+
# |label|            features|           assembled|        polyfeatures|        prediction|
# +-----+--------------------+--------------------+--------------------+------------------+
# | 24.0|[0.00632,18.0,2.3...|[0.00632,18.0,2.3...|[0.00632,3.99424E...| 25.86813228073403|
# | 21.6|[0.02731,0.0,7.07...|[0.02731,0.0,7.07...|[0.02731,7.458361...|24.282222519858333|
# | 34.7|[0.02729,0.0,7.07...|[0.02729,0.0,7.07...|[0.02729,7.447440...| 32.57933183456977|
# | 33.4|[0.03237,0.0,2.18...|[0.03237,0.0,2.18...|[0.03237,0.001047...| 31.70426384119063|
# | 36.2|[0.06905,0.0,2.18...|[0.06905,0.0,2.18...|[0.06905,0.004767...| 32.73288697359488|
# | 28.7|[0.02985,0.0,2.18...|[0.02985,0.0,2.18...|[0.02985,8.910225...| 26.08027158070393|
# | 22.9|[0.08829,12.5,7.8...|[0.08829,12.5,7.8...|[0.08829,0.007795...| 20.57022132072221|
# | 27.1|[0.14455,12.5,7.8...|[0.14455,12.5,7.8...|[0.14455,0.020894...|19.961291460003373|
# | 16.5|[0.21124,12.5,7.8...|[0.21124,12.5,7.8...|[0.21124,0.044622...|15.702740253359373|
# | 18.9|[0.17004,12.5,7.8...|[0.17004,12.5,7.8...|[0.17004,0.028913...|18.813120951399434|
# | 15.0|[0.22489,12.5,7.8...|[0.22489,12.5,7.8...|[0.22489,0.050575...|21.522339713954167|
# | 18.9|[0.11747,12.5,7.8...|[0.11747,12.5,7.8...|[0.11747,0.013799...| 19.16802960412294|
# | 21.7|[0.09378,12.5,7.8...|[0.09378,12.5,7.8...|[0.09378,0.008794...|21.108549586840883|
# | 20.4|[0.62976,0.0,8.14...|[0.62976,0.0,8.14...|[0.62976,0.396597...|17.475093351433742|
# | 18.2|[0.63796,0.0,8.14...|[0.63796,0.0,8.14...|[0.63796,0.406992...|16.776025755560724|
